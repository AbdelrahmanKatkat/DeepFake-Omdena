{"cells":[{"cell_type":"markdown","metadata":{"id":"bwJWYAPLRGii"},"source":["<font color='red' size='5px'/> Data Preprocessing<font/>"],"id":"bwJWYAPLRGii"},{"cell_type":"markdown","metadata":{"id":"AX17gv65RGir"},"source":["<font color='blue' size='5px'/> Introduction<font/>"],"id":"AX17gv65RGir"},{"cell_type":"markdown","metadata":{"id":"xYxmWSPcRGir"},"source":["#  Overview "],"id":"xYxmWSPcRGir"},{"cell_type":"markdown","metadata":{"id":"42FoODtGRGis"},"source":["We have decleared the dataset to be [Dataset](https://iplab.dmi.unict.it/Deepfakechallenge/). Now we want to choose how to perform preprocessing on this dataset. We've decided that we are going to split into two teams each team will be working with different methedology. The first will use Keras and the other will use Albumentations libarary"],"id":"42FoODtGRGis"},{"cell_type":"markdown","metadata":{"id":"q6cMSLsURGit"},"source":["We want to use Data augmentation techniques to increase the size and diversity of a dataset by applying transformations to the existing data. This helps to prevent overfitting and improves the generalization of the model."],"id":"q6cMSLsURGit"},{"cell_type":"markdown","metadata":{"id":"slmYlXICRGit"},"source":["#  Literature Review"],"id":"slmYlXICRGit"},{"cell_type":"markdown","metadata":{"id":"F-bLZiBGRGiu"},"source":["## 1 The Face Deepfake Detection Challenge"],"id":"F-bLZiBGRGiu"},{"cell_type":"markdown","metadata":{"id":"jedDmAULRGiu"},"source":["### 1.1 Overview"],"id":"jedDmAULRGiu"},{"cell_type":"markdown","metadata":{"id":"FgZfG8IHRGiv"},"source":["1. Data Collection: The authors collected a large dataset of real and fake face images from various sources, including publicly available datasets and images generated using deepfake techniques.\n","\n","2. Data Preparation: The dataset was split into training, validation, and testing sets. The authors also applied various data augmentation techniques to increase the size of the training set and improve the generalization of the model.\n","\n","3. Baseline Model: The authors developed a baseline deep learning model for face detection using a combination of convolutional neural networks (CNNs) and recurrent neural networks (RNNs). The model was trained on the training set and evaluated on the validation set.\n","\n","4. Challenge: The authors organized a challenge in which participants were asked to develop their own deepfake detection models and submit their results for evaluation. The challenge provided a platform for researchers to compare their methods and identify the most effective techniques.\n","\n","5. Evaluation: The authors evaluated the performance of the participating models using various metrics, including precision, recall, and F1 score. They also compared the performance of the top-performing models with the baseline model."],"id":"FgZfG8IHRGiv"},{"cell_type":"markdown","metadata":{"id":"BELVNGd7RGiw"},"source":["### 1.2 Data Augmentation Techniques"],"id":"BELVNGd7RGiw"},{"cell_type":"markdown","metadata":{"id":"rg2FejhdRGiw"},"source":["1. Horizontal flipping:\n","    - Horizontally flipped some of the images in the dataset to create mirror images. \n","    - This increases the diversity of the training set and makes the model more robust to variations in facial orientation.\n","\n","2. Random cropping:\n","    - Cropped some of the images in the dataset to create new images with different sizes and aspect ratios.\n","    - To recognize partially obscured or have different proportions.\n","\n","3. Gaussian noise: \n","    - The authors added random Gaussian noise to some of the images in the dataset to simulate noise that might be present in real-world images. \n","    - This helps the model to learn to recognize faces in noisy or low-quality images.\n","\n","4. Random brightness and contrast: \n","    - Randomly adjusted the brightness and contrast of some of the images in the dataset to simulate variations in lighting conditions. \n","    - This helps the model to learn to recognize faces in different lighting conditions.\n","    -  For example, the brightness can be adjusted by adding a random value between -50 and 50 to the pixel values, while the contrast can be adjusted by multiplying the pixel values by a random value between 0.5 and 1.5."],"id":"rg2FejhdRGiw"},{"cell_type":"markdown","source":["### 1.3 Different Values used\n","- size= (128,128)\n","- Batch size= 64\n","- Data Augmentation parameters:\n","  - Image compression: images were compressed with the JPEG algorithm at a quality factor picked uniformaly in the range[50:99]\n","  - Noise: Images were corrupted with additive Gaussian noise with variable limit in ragne[10,50]\n","  - Blurring: Gaussian blurring was applied to the images, with blur a limit of 3 and sigma limit of 0"],"metadata":{"id":"W10a1SY6RurE"},"id":"W10a1SY6RurE"},{"cell_type":"markdown","metadata":{"id":"06b3yHuXRGix"},"source":["## 2 Investigating the Impact of Pre-processing and Prediction Aggregation on the Deepfake Detection Task"],"id":"06b3yHuXRGix"},{"cell_type":"markdown","metadata":{"id":"2dQImJG1RGiy"},"source":["## 2.1 Overview"],"id":"2dQImJG1RGiy"},{"cell_type":"markdown","metadata":{"id":"uFXoCGGERGiy"},"source":["The paper focuses on exploring the impact of pre-processing and prediction aggregation techniques on the deepfake detection task. "],"id":"uFXoCGGERGiy"},{"cell_type":"markdown","metadata":{"id":"NGc7-ydXRGiz"},"source":["The authors use a state-of-the-art deep learning model called XceptionNet for the detection task and evaluate the model's performance on three publicly available datasets:\n","1. Celeb-DF,\n","2. DeepFake-TIMIT,\n","3. FaceForensics++."],"id":"NGc7-ydXRGiz"},{"cell_type":"markdown","metadata":{"id":"qkQTKuIpRGiz"},"source":["## 2.2 Preprocessing Techniques"],"id":"qkQTKuIpRGiz"},{"cell_type":"markdown","metadata":{"id":"sMKM51EGRGiz"},"source":["He studied the performance change due to:\n","- Grayscale conversion,\n","- Resizing,\n","- Histogram equalization,\n","- median filtering"],"id":"sMKM51EGRGiz"},{"cell_type":"markdown","metadata":{"id":"NxS2R2IQRGi0"},"source":["He found out:\n","- grayscale conversion and histogram equalization significantly improve the detection performance on all three datasets,\n","- while resizing and median filtering have a limited impact or even worsen the performance."],"id":"NxS2R2IQRGi0"},{"cell_type":"markdown","metadata":{"id":"sUo0Qmd9RGi1"},"source":["## 2.3 Aggregation Techniques"],"id":"sUo0Qmd9RGi1"},{"cell_type":"markdown","metadata":{"id":"S_jN0-_4RGi1"},"source":["The authors also explore three prediction aggregation techniques, namely majority voting, averaging, and stacking, to combine the predictions of multiple models or multiple runs of the same model to further enhance the accuracy."],"id":"S_jN0-_4RGi1"},{"cell_type":"markdown","metadata":{"id":"ntzZEnMbRGi2"},"source":["He found out:\n","- The results show that majority voting and averaging are effective and efficient techniques for prediction aggregation,\n","- while stacking has a higher computational cost and may not always improve the performanc"],"id":"ntzZEnMbRGi2"},{"cell_type":"markdown","metadata":{"id":"4MpTALR-RGi2"},"source":["# 3 An Improved Dense CNN Architecture for Deepfake Image Detection"],"id":"4MpTALR-RGi2"},{"cell_type":"markdown","metadata":{"id":"zYSjo6PQRGi2"},"source":["## 3.1 Overview"],"id":"zYSjo6PQRGi2"},{"cell_type":"code","execution_count":null,"metadata":{"id":"uLPwANFdRGi5"},"outputs":[],"source":[],"id":"uLPwANFdRGi5"},{"cell_type":"code","execution_count":null,"metadata":{"id":"6pyRSnKDRGi5"},"outputs":[],"source":[],"id":"6pyRSnKDRGi5"},{"cell_type":"code","execution_count":null,"metadata":{"id":"f9U6YiU5RGi5"},"outputs":[],"source":[],"id":"f9U6YiU5RGi5"},{"cell_type":"markdown","metadata":{"id":"5eskyxHXRGi6"},"source":["<font color='blue' size='5px'/> Meetings<font/>"],"id":"5eskyxHXRGi6"},{"cell_type":"markdown","source":["# 1 Conclusion"],"metadata":{"id":"fQPElmbxUNLb"},"id":"fQPElmbxUNLb"},{"cell_type":"markdown","metadata":{"id":"SVSY5d2HRGi6"},"source":["We conducted a meeting to discuss parameters used in each of the papers and the conclusions werwe:\n","- Normalization will not performed in this phase, but we will make it in training.\n","- We will need to perform image compression to reduce the size during training.\n","- Do small blurring, as we already applied image compression. \n","- We will use MTCNN for face detection, which will improve our performance\n","- We have 2 parts in Data some generated by GAN, and others are real, so we will augment real data by value 1 to 3 and fake data by value 1 to 6\n","- We will split data into train, test, val by random."],"id":"SVSY5d2HRGi6"},{"cell_type":"markdown","source":["# 2 Parameters Used"],"metadata":{"id":"s2L9-IM7RGi8"},"id":"s2L9-IM7RGi8"},{"cell_type":"markdown","source":["- Both teams (keras and albumentations team) will work accordingly to create augmented images of each folder (that is CelebA, FFHQ, ATTGAN,...)  separately.\n","- Few augmentation parameters to be followed:\n","  - Resize images to = (160, 160) (must be same for both teams)\n","  - Random rotation (0 to 360)\n","  - Horizontal or Vertical flip\n","  - Shear Range(0.2), zoom range(0.2) (can vary for both teams)\n","  - Height or width shift by 0.2 (can vary)\n","  - Blackout of either eyes, face or mouth (very imp)\n","  - Image compression (lower limit = 50, upper limit = 100)\n","  - Gaussian Blurring or Gaussian Noise (either of both or together also can be used, just make sure the image is not degraded or blurred a lot!) \n","  - People working on Deepfake image folder, keep value of blurring and noise less as images are already blurred! Just have a look at images properly before applying Blur or Noise\n","  - Either Random brightness, saturation, hue contrast (comes under ColorJitter in albumentations)\n","  - Avoid cropping or even if you do, use only central crop and that too with low probability. (to avoid information loss)\n","- (Using too many augmentations can even degrade the dataset!) \n","Don’t Normalise/rescale images right now. This will be done later while model training.\n","- Exploit features like OneOf provided in Albumentations library.\n","(link → Composition API (core.composition) - Albumentations Documentation )\n","\n","- Reference :\n","\n","  1.[Overview and visualization of pixel-level transforms from albumentations package|Data-science-blog](https://tugot17.github.io/data-science-blog/albumentations/data-augmentation/tutorial/2020/09/20/Pixel-level-transforms-using-albumentations-package.html) \n","      \n","  2.[Index - Albumentations Documentation](https://albumentations.ai/docs/api_reference/augmentations/)\n","  \n","  3.Very Important read before setting values of probabilities and before using OneOf :\n","\n","  [Setting probabilities for transforms - Albumentations Documentation ](https://albumentations.ai/docs/getting_started/setting_probabilities/)\n"],"metadata":{"id":"4P1EDL9cUSY8"},"id":"4P1EDL9cUSY8"},{"cell_type":"markdown","metadata":{"id":"o1oifqTpRGi8"},"source":["<font color='blue' size='5px'/> Implement in Project<font/>"],"id":"o1oifqTpRGi8"},{"cell_type":"markdown","metadata":{"id":"AxnJkQ0cRGi8"},"source":["# 1 Packages"],"id":"AxnJkQ0cRGi8"},{"cell_type":"code","source":["!pip install facenet-pytorch\n","!pip install -U albumentations\n","from PIL import Image\n","from facenet_pytorch.models.mtcnn import MTCNN\n","from google.colab.patches import cv2_imshow\n","from random import randint\n","import random\n","import zipfile\n","import cv2\n","import dlib"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4oQc9n1qWUZq","executionInfo":{"status":"ok","timestamp":1678969106941,"user_tz":-120,"elapsed":27791,"user":{"displayName":"Abdelrahman Katkat","userId":"13800345600658892031"}},"outputId":"eb652b3a-0730-4769-c015-292175db5b16"},"id":"4oQc9n1qWUZq","execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting facenet-pytorch\n","  Downloading facenet_pytorch-2.5.2-py3-none-any.whl (1.9 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from facenet-pytorch) (2.25.1)\n","Requirement already satisfied: pillow in /usr/local/lib/python3.9/dist-packages (from facenet-pytorch) (8.4.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from facenet-pytorch) (1.22.4)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.9/dist-packages (from facenet-pytorch) (0.14.1+cu116)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->facenet-pytorch) (2022.12.7)\n","Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.9/dist-packages (from requests->facenet-pytorch) (4.0.0)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->facenet-pytorch) (2.10)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->facenet-pytorch) (1.26.15)\n","Requirement already satisfied: torch==1.13.1 in /usr/local/lib/python3.9/dist-packages (from torchvision->facenet-pytorch) (1.13.1+cu116)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torchvision->facenet-pytorch) (4.5.0)\n","Installing collected packages: facenet-pytorch\n","Successfully installed facenet-pytorch-2.5.2\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: albumentations in /usr/local/lib/python3.9/dist-packages (1.2.1)\n","Collecting albumentations\n","  Downloading albumentations-1.3.0-py3-none-any.whl (123 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.5/123.5 KB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: PyYAML in /usr/local/lib/python3.9/dist-packages (from albumentations) (6.0)\n","Requirement already satisfied: numpy>=1.11.1 in /usr/local/lib/python3.9/dist-packages (from albumentations) (1.22.4)\n","Requirement already satisfied: qudida>=0.0.4 in /usr/local/lib/python3.9/dist-packages (from albumentations) (0.0.4)\n","Requirement already satisfied: opencv-python-headless>=4.1.1 in /usr/local/lib/python3.9/dist-packages (from albumentations) (4.7.0.72)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.9/dist-packages (from albumentations) (1.10.1)\n","Requirement already satisfied: scikit-image>=0.16.1 in /usr/local/lib/python3.9/dist-packages (from albumentations) (0.19.3)\n","Requirement already satisfied: scikit-learn>=0.19.1 in /usr/local/lib/python3.9/dist-packages (from qudida>=0.0.4->albumentations) (1.2.2)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from qudida>=0.0.4->albumentations) (4.5.0)\n","Requirement already satisfied: pillow!=7.1.0,!=7.1.1,!=8.3.0,>=6.1.0 in /usr/local/lib/python3.9/dist-packages (from scikit-image>=0.16.1->albumentations) (8.4.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from scikit-image>=0.16.1->albumentations) (23.0)\n","Requirement already satisfied: imageio>=2.4.1 in /usr/local/lib/python3.9/dist-packages (from scikit-image>=0.16.1->albumentations) (2.9.0)\n","Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.9/dist-packages (from scikit-image>=0.16.1->albumentations) (3.0)\n","Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from scikit-image>=0.16.1->albumentations) (1.4.1)\n","Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.9/dist-packages (from scikit-image>=0.16.1->albumentations) (2023.2.28)\n","Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from scikit-learn>=0.19.1->qudida>=0.0.4->albumentations) (1.1.1)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn>=0.19.1->qudida>=0.0.4->albumentations) (3.1.0)\n","Installing collected packages: albumentations\n","  Attempting uninstall: albumentations\n","    Found existing installation: albumentations 1.2.1\n","    Uninstalling albumentations-1.2.1:\n","      Successfully uninstalled albumentations-1.2.1\n","Successfully installed albumentations-1.3.0\n"]}]},{"cell_type":"code","source":["import torch\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import pandas as pd\n","import math\n","import albumentations as A\n","import os"],"metadata":{"id":"PKqe_IyuWt1a","executionInfo":{"status":"ok","timestamp":1678969115922,"user_tz":-120,"elapsed":2018,"user":{"displayName":"Abdelrahman Katkat","userId":"13800345600658892031"}}},"id":"PKqe_IyuWt1a","execution_count":2,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"uxmouoCZW-E5"},"id":"uxmouoCZW-E5","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"74p_qWBXRGi8"},"source":["# 2 Explore Data"],"id":"74p_qWBXRGi8"},{"cell_type":"code","source":["from google.colab import drive \n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FbKTyWoCW9i5","executionInfo":{"status":"ok","timestamp":1678969233495,"user_tz":-120,"elapsed":47499,"user":{"displayName":"Abdelrahman Katkat","userId":"13800345600658892031"}},"outputId":"32b445d1-aeb4-4ab3-d99b-3b48ae4a1c0f"},"id":"FbKTyWoCW9i5","execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["celebA_path = \"/content/drive/MyDrive/Omdena DeepFake Image Detection (preprocessed data)/Train/0-CelebA\"\n","ffhq_path = \"/content/drive/MyDrive/Omdena DeepFake Image Detection (preprocessed data)/Train/0-FFHQ\"\n","attgan_path = \"/content/drive/MyDrive/Omdena DeepFake Image Detection (preprocessed data)/Train/1-ATTGAN\"\n","gdwct_path = \"/content/drive/MyDrive/Omdena DeepFake Image Detection (preprocessed data)/Train/1-GDWCT\"\n","stylegan_path = \"/content/drive/MyDrive/Omdena DeepFake Image Detection (preprocessed data)/Train/1-STYLEGAN\"\n","stylegan2_path = \"/content/drive/MyDrive/Omdena DeepFake Image Detection (preprocessed data)/Train/1-STYLEGAN2/1-STYLEGAN2\"\n","stargan_path = \"/content/drive/MyDrive/Omdena DeepFake Image Detection (preprocessed data)/Train/1-StarGAN\""],"metadata":{"id":"XIubbSwIXugp","executionInfo":{"status":"ok","timestamp":1678969338133,"user_tz":-120,"elapsed":3,"user":{"displayName":"Abdelrahman Katkat","userId":"13800345600658892031"}}},"id":"XIubbSwIXugp","execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"N9ec6YkpRGi8"},"source":["# 3 Freature Engineering "],"id":"N9ec6YkpRGi8"},{"cell_type":"markdown","source":["## 3.1 Black Out function"],"metadata":{"id":"IpqHrdtQX0Yw"},"id":"IpqHrdtQX0Yw"},{"cell_type":"code","source":["def blackout_eyes(img, boxes, landmarks):\n","  left_eye_x, left_eye_y = landmarks[0][0][0], landmarks[0][0][1]\n","  right_eye_x, right_eye_y = landmarks[0][1][0], landmarks[0][1][1]\n","\n","  box_width, box_height = boxes[0][2]*0.8, boxes[0][3]*0.1              \n","  #defining width and height of black rectangle to be created\n","  width_margin, height_margin = 0.1*box_width, box_height*0.4\n","  ## old--->> width_margin, height_margin = abs(left_eye_x - right_eye_x)*0.25, 10\n","  #width_margin, height_margin = width*0.8, box_height*0.1\n","  if(landmarks[0][0][1] >= landmarks[0][1][1]):\n","    start = (math.ceil(left_eye_x - width_margin), math.ceil(left_eye_y - height_margin))\n","    end = (math.ceil(right_eye_x + width_margin), math.ceil(right_eye_y + height_margin))\n","  else:\n","    end = (math.ceil(left_eye_x - width_margin), math.ceil(left_eye_y - height_margin))\n","    start = (math.ceil(right_eye_x + width_margin), math.ceil(right_eye_y + height_margin))\n","  cv2.rectangle(img, start, end, (0,0,0),-1)\n","  return img\n","\n","def blackout_nose(img, boxes, landmarks):\n","  nose_x, nose_y = landmarks[0][2][0], landmarks[0][2][1]\n","  box_width, box_height = boxes[0][2]*0.8, boxes[0][3]*0.1\n","  width_margin, height_margin = 0.15*box_width, box_height*0.5\n","  start = (math.ceil(nose_x - width_margin), math.ceil(nose_y - height_margin*1.5))\n","  end = (math.ceil(nose_x + width_margin), math.ceil(nose_y + height_margin))\n","  cv2.rectangle(img, start, end, (0,0,0),-1)\n","  return img\n","\n","def blackout_mouth(img, boxes, landmarks):\n","  left_mouth_x, left_mouth_y = landmarks[0][3][0], landmarks[0][3][1]\n","  right_mouth_x, right_mouth_y = landmarks[0][4][0], landmarks[0][4][1]\n","  box_width, box_height = boxes[0][2]*0.5, boxes[0][3]*0.15\n","  #defining width and height of black rectangle to be created\n","  width_margin, height_margin = 0.1*box_width, box_height*0.35\n","  ## old--->> width_margin, height_margin = abs(left_eye_x - right_eye_x)*0.25, 10\n","  #width_margin, height_margin = width*0.8, box_height*0.1\n","  start = (math.ceil(left_mouth_x - width_margin), math.ceil(left_mouth_y - height_margin))\n","  end = (math.ceil(right_mouth_x + width_margin), math.ceil(right_mouth_y + height_margin))\n","  cv2.rectangle(img, start, end, (0,0,0),-1)\n","  return img "],"metadata":{"id":"t-8w_u9UXfWQ","executionInfo":{"status":"ok","timestamp":1678969294036,"user_tz":-120,"elapsed":918,"user":{"displayName":"Abdelrahman Katkat","userId":"13800345600658892031"}}},"id":"t-8w_u9UXfWQ","execution_count":4,"outputs":[]},{"cell_type":"code","source":["def blackout(img, boxes, probs, landmarks):\n","  if not probs[0]:     #incase no face detected by MTCNN\n","    return img\n","\n","  if probs[0] < 0.95:  #in case MTCNN not sure about exact face landmarks, can be due to blurring\n","    return img\n","\n","  m = randint(0, 3)  ##Random number\n","  \n","  if m==0:\n","    return blackout_eyes(img, boxes, landmarks)\n","  \n","  elif m==1:\n","    return blackout_nose(img, boxes, landmarks)\n","\n","  else:\n","    return blackout_mouth(img, boxes, landmarks)"],"metadata":{"id":"XyMRcqyHXk9O","executionInfo":{"status":"ok","timestamp":1678969298621,"user_tz":-120,"elapsed":4,"user":{"displayName":"Abdelrahman Katkat","userId":"13800345600658892031"}}},"id":"XyMRcqyHXk9O","execution_count":5,"outputs":[]},{"cell_type":"code","source":["def mtcnn_params(img):\n","    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","    mtcnn = MTCNN(keep_all=True, device=device)\n","    boxes, probs, landmarks = mtcnn.detect(img, landmarks=True)\n","    return boxes, probs, landmarks"],"metadata":{"id":"A65KvwUZXnEI","executionInfo":{"status":"ok","timestamp":1678969306871,"user_tz":-120,"elapsed":736,"user":{"displayName":"Abdelrahman Katkat","userId":"13800345600658892031"}}},"id":"A65KvwUZXnEI","execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"flNslQsRRGi9"},"source":["# 4 Preprocessing"],"id":"flNslQsRRGi9"},{"cell_type":"markdown","source":["## 4.1 Perform Augmentation with Albumentation"],"metadata":{"id":"z-6mndFgRGi-"},"id":"z-6mndFgRGi-"},{"cell_type":"markdown","source":["## 4.2 Experiment "],"metadata":{"id":"TcoY1JEXX8Fz"},"id":"TcoY1JEXX8Fz"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.6"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}