{"cells":[{"cell_type":"markdown","source":["<font color='red' size='5px'/> Deploy ML model<font/>"],"metadata":{"id":"hroMgZFjmBlW"},"id":"hroMgZFjmBlW"},{"cell_type":"markdown","id":"88de6987","metadata":{"id":"88de6987"},"source":["<font color='blue' size='5px'/> Deploy <font/>"]},{"cell_type":"markdown","id":"cb2ae999","metadata":{"id":"cb2ae999"},"source":["# 1 Overview "]},{"cell_type":"markdown","source":["There are several options to deploy a machine learning model, depending on your requirements, resources, and expertise."],"metadata":{"id":"-m9P9N3CmWmK"},"id":"-m9P9N3CmWmK"},{"cell_type":"markdown","source":["1. API/web service deployment: This method involves deploying a RESTful API that exposes your machine learning model as a web service, which can receive input data and return the model's predictions in real-time.\n","  - The API can be deployed on a web server, such as AWS EC2 or AWS Lambda, and can be accessed by other applications or services over the internet. \n","  - This method is useful for applications that require real-time prediction, such as chatbots, recommendation systems, and fraud detection.\n","\n","2. Edge deployment: This method involves deploying your machine learning model to a device or edge server, such as a smartphone, a smart camera, or an IoT gateway, that can perform inference locally without sending data to the cloud.\n","  - Edge deployment can offer several benefits, such as reduced latency, increased privacy, and improved reliability, especially in scenarios where network connectivity is limited or intermittent.\n","\n","3. Batch deployment: This method involves processing large batches of data offline, such as overnight or on weekends, using a batch processing service, such as AWS Batch or AWS Glue. \n","  - Batch deployment is useful for applications that require processing large volumes of data, such as image or text classification, and can offer cost savings by utilizing compute resources only when needed."],"metadata":{"id":"W1WdOqhrZrTJ"},"id":"W1WdOqhrZrTJ"},{"cell_type":"markdown","source":["# 2 Terminology"],"metadata":{"id":"lY9v1gyhZCpu"},"id":"lY9v1gyhZCpu"},{"cell_type":"markdown","source":["## 2.1 RESTful API\n","\n","A RESTful API is a type of web service that is designed to be flexible, scalable, and easy to use. \n","\n","- The term \"REST\" stands for Representational State Transfer, which is a set of architectural principles for building web services.\n","\n","- A RESTful API typically uses HTTP requests to perform operations on resources, such as:\n","  - Retrieving data\n","  - Creating new resources,\n","  - Updating existing resources, \n","  - Deleting resources. \n","- Each resource is identified by a unique URL, and clients can use HTTP methods such as GET, POST, PUT, and DELETE to perform operations on those resources."],"metadata":{"id":"qQxn6ZK3ZH3b"},"id":"qQxn6ZK3ZH3b"},{"cell_type":"markdown","source":["## 2.2 End Point\n","\n","Endpoints are the URLs that clients use to interact with the API. The endpoints typically represent resources or actions that the API can perform.\n","\n","- Each endpoint typically has a unique URL, HTTP method (such as GET, POST, PUT, DELETE), and parameters or data required for the request. The response returned by the API typically includes a status code (such as 200 OK or 404 Not Found) and a data format (such as JSON or XML) that contains the results of the request.\n","\n","- For example, let's say you have a machine learning model that can classify images of dogs and cats. You might define the following endpoints for your RESTful API:\n","\n","  - /classify - accepts a POST request with an image file, and returns the predicted class (dog or cat) as a JSON response.\n","  - /train - accepts a POST request with training data, and trains the model.\n","  - /metrics - accepts a GET request and returns the current evaluation metrics for the model.\n","  - /healthcheck - accepts a GET request and returns a response indicating whether the API is healthy and available."],"metadata":{"id":"fHvW_FgfbUI6"},"id":"fHvW_FgfbUI6"},{"cell_type":"markdown","source":["## 2.3 JSON response \n","\n","JSON response is a data format used for sending and receiving data in web applications. JSON stands for JavaScript Object Notation and is a lightweight data interchange format that is easy for humans to read and write, and easy for machines to parse and generate.\n","\n","- A JSON response typically consists of a string of key-value pairs that represent data in a structured format. The keys are strings and the values can be any of the following data types: string, number, boolean, array, or object.\n","\n","- JSON is widely used in web applications for transmitting data between the server and the client in a standardized and easily parsable format. It is also used for exchanging data between different systems and applications.\n"],"metadata":{"id":"YKd9S8nDcVOa"},"id":"YKd9S8nDcVOa"},{"cell_type":"markdown","source":["## 2.4 Dockerfile\n","\n","A Dockerfile is a text file that contains a set of instructions for building a Docker image. It is used to automate the process of building, deploying, and running applications in a Docker container.\n","\n","- A Dockerfile typically includes instructions for building a base image, configuring the environment, installing dependencies, and copying files or code into the container.\n","- Each instruction in the Dockerfile creates a new layer in the container image, which allows for efficient and incremental builds."],"metadata":{"id":"K5eagtrachUB"},"id":"K5eagtrachUB"},{"cell_type":"code","source":["# Use an official Python runtime as a parent image\n","FROM python:3.8-slim-buster\n","\n","# Set the working directory to /app\n","WORKDIR /app\n","\n","# Copy the current directory contents into the container at /app\n","COPY . /app\n","\n","# Install any needed packages specified in requirements.txt\n","RUN pip install --trusted-host pypi.python.org -r requirements.txt\n","\n","# Make port 80 available to the world outside this container\n","EXPOSE 80\n","\n","# Define environment variable\n","ENV NAME World\n","\n","# Run app.py when the container launches\n","CMD [\"python\", \"app.py\"]"],"metadata":{"id":"G6ol1Y8fdvfp"},"id":"G6ol1Y8fdvfp","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["This Dockerfile does the following:\n","\n","- Starts with an official Python 3.8 slim version image as the base image.\n","- Sets the working directory in the container to /app.\n","- Copies the contents of the current directory into the container at /app.\n","- Installs the dependencies specified in the requirements.txt file using pip.\n","- Exposes port 80 to allow external access.\n","- Sets an environment variable called NAME to \"World\".\n","- Specifies that the command to run when the container launches is to execute the app.py file with Python.\n","- Once the Dockerfile is created, it can be used to build a Docker image using the docker build command."],"metadata":{"id":"pe30CHN8dGZh"},"id":"pe30CHN8dGZh"},{"cell_type":"markdown","source":["## 2.5 Docker image:"],"metadata":{"id":"faA61kqvdZih"},"id":"faA61kqvdZih"},{"cell_type":"markdown","source":["A Docker image is a lightweight, standalone, and executable package that includes everything needed to run a piece of software, including the code, runtime, libraries, tools, and settings.\n","\n","- Docker images are created from a Dockerfile, which specifies the configuration and dependencies of the software to be packaged. Each instruction in the Dockerfile creates a new layer in the image, which allows for efficient storage and sharing of images.\n","\n","- Docker images can be stored in a registry, such as Docker Hub, which allows users to easily share and distribute their images. Docker images can be pulled from a registry onto a host machine and used to run a container, which is an instance of the image running as a process.\n","\n","- Docker images provide a portable and consistent environment for running software, regardless of the host system or infrastructure. They are widely used in DevOps and containerization to package and deploy applications in a reproducible and scalable way."],"metadata":{"id":"yWOKqLOfdiNJ"},"id":"yWOKqLOfdiNJ"},{"cell_type":"markdown","source":["# 3 AWS"],"metadata":{"id":"r1avVYsRnZQR"},"id":"r1avVYsRnZQR"},{"cell_type":"markdown","source":["## 2.1 Steps"],"metadata":{"id":"_a-1-5l7nlha"},"id":"_a-1-5l7nlha"},{"cell_type":"markdown","source":["1. Train and save your machine learning model using a framework such as TensorFlow, PyTorch, or Scikit-Learn.\n","2. Create a Flask web application that will serve as the API for your model. You can define the routes and endpoints for the API in the Flask application.\n","3. Load the trained model in the Flask application and define the logic for making predictions based on the input data.\n","4. Build a Docker image for your Flask application using a Dockerfile. The Dockerfile should specify the dependencies, environment variables, and other configurations needed to run the application.\n","5. Push the Docker image to a container registry such as Docker Hub or Amazon ECR.\n","6. Deploy the Docker container to a cloud-based service such as Amazon ECS, AWS Fargate, or Kubernetes."],"metadata":{"id":"DXJtt0ukeKcC"},"id":"DXJtt0ukeKcC"},{"cell_type":"markdown","source":["1. Exporting your model in a format compatible with AWS, such as TensorFlow SavedModel, ONNX, or MXNet, and creating a requirements file for the dependencies.\n","\n","2. Create an Amazon SageMaker instance: Amazon SageMaker is a fully managed service that provides a pre-configured environment for deploying and managing machine learning models. \n","\n","3. Upload your model and dependencies: Once you have created an Amazon SageMaker instance, you can upload your model and dependencies to it. You can do this using the AWS Management Console, AWS CLI, or AWS SDK.\n","\n","4. Deploy your model as an endpoint: Once your model and dependencies are uploaded to Amazon SageMaker, you can deploy your model as an endpoint. This creates a REST API that can be used to interact with your model. \n","\n","5. You can configure the endpoint settings, such as the instance type, number of instances, and auto-scaling policies, to optimize the performance and cost of your model.\n","\n","6. Test and monitor your model: After deploying your model, you can test it using sample inputs and evaluate its performance. You can also monitor the endpoint metrics, such as latency, throughput, and error rate, using Amazon CloudWatch.\n","\n","7. Update and re-deploy your model: As your model evolves or your requirements change, you can update and re-deploy your model using the same process. This allows you to iterate quickly and maintain the scalability and reliability of your model."],"metadata":{"id":"he3MyLKjnpBD"},"id":"he3MyLKjnpBD"},{"cell_type":"markdown","id":"bd2bc40a","metadata":{"id":"bd2bc40a"},"source":["# 4 Online Examples"]},{"cell_type":"code","source":["## "],"metadata":{"id":"IuCGmWbLdhCb"},"id":"IuCGmWbLdhCb","execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"27558Kj8dedi"},"id":"27558Kj8dedi","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<font color='blue' size='5px'/>Implement In Project <font/>"],"metadata":{"id":"YX5HezXwnVQM"},"id":"YX5HezXwnVQM"},{"cell_type":"markdown","id":"30b2866a","metadata":{"id":"30b2866a"},"source":["# 1 Packages"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TceqHSXprGhg","executionInfo":{"status":"ok","timestamp":1680668983924,"user_tz":-120,"elapsed":21670,"user":{"displayName":"Abdelrahman Katkat","userId":"13800345600658892031"}},"outputId":"963bd8c5-2462-418c-b892-9037b60f8610"},"id":"TceqHSXprGhg","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","%matplotlib inline"],"metadata":{"id":"VgBWPHsWrV-3"},"id":"VgBWPHsWrV-3","execution_count":null,"outputs":[]},{"cell_type":"code","source":["import tensorflow as tf\n","import tensorflow_datasets as tfd"],"metadata":{"id":"LQTCsS5oref9"},"id":"LQTCsS5oref9","execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"jXeug7U2r1RY"},"id":"jXeug7U2r1RY","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"34b795f4","metadata":{"id":"34b795f4"},"source":["# 2 Explore Data"]},{"cell_type":"markdown","id":"f86e8755","metadata":{"id":"f86e8755"},"source":["# 3 Freature Engineering "]},{"cell_type":"markdown","id":"a0241c6f","metadata":{"id":"a0241c6f"},"source":["## 3.1 Missing Data"]},{"cell_type":"markdown","id":"69a2dfb4","metadata":{"id":"69a2dfb4"},"source":["## 3.2 Dummy Variables"]},{"cell_type":"markdown","id":"60fed7c1","metadata":{"id":"60fed7c1"},"source":["## 3.3 Drop Data"]},{"cell_type":"markdown","id":"45b78dd0","metadata":{"id":"45b78dd0"},"source":["# 4 Preprocessing"]},{"cell_type":"markdown","id":"1b005866","metadata":{"id":"1b005866"},"source":["## 4.1 Split Data"]},{"cell_type":"markdown","id":"024087b0","metadata":{"id":"024087b0"},"source":["## 4.2 Scalling "]},{"cell_type":"markdown","id":"e64d7ead","metadata":{"id":"e64d7ead"},"source":["# 5 Training"]},{"cell_type":"markdown","id":"2a035f96","metadata":{"id":"2a035f96"},"source":["# 6 Prediction"]},{"cell_type":"markdown","id":"77423850","metadata":{"id":"77423850"},"source":["# 7 Evaluation"]},{"cell_type":"markdown","source":["# 8 Deployment"],"metadata":{"id":"tlNClJ_HruDW"},"id":"tlNClJ_HruDW"},{"cell_type":"markdown","source":["## 8.1 Save The Moel"],"metadata":{"id":"2prffAGetB1L"},"id":"2prffAGetB1L"},{"cell_type":"markdown","source":["## 8.2 Create Flask Web Applicaiton"],"metadata":{"id":"VxzZuAxPtByj"},"id":"VxzZuAxPtByj"},{"cell_type":"code","source":["from flask import Flask, request, jsonify"],"metadata":{"id":"UY77yAN8tBm2"},"id":"UY77yAN8tBm2","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"e52ab383","metadata":{"id":"e52ab383"},"outputs":[],"source":["app=Flask(__name__)\n","@app.route('/predict', methods=['POST'])\n","def predict():\n","    data = request.get_json()\n","    # use data to make predictions with your machine learning model\n","    predictions = model.predict(data)\n","    # return predictions as a JSON response\n","    return jsonify(predictions)"]},{"cell_type":"markdown","source":["In this example code, the /predict endpoint accepts a POST request containing JSON data. The data is passed to your machine learning model to make predictions, and the predictions are returned as a JSON response."],"metadata":{"id":"jj2f3pHltpr-"},"id":"jj2f3pHltpr-"},{"cell_type":"markdown","source":["## 8.3 Load ML model In Flask"],"metadata":{"id":"SzMgehC3tzqP"},"id":"SzMgehC3tzqP"},{"cell_type":"code","source":["from tensorflow.keras.models import load_"],"metadata":{"id":"CgHJop9Ct5IE"},"id":"CgHJop9Ct5IE","execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = load_model('/content/drive/MyDrive/Colab Notebooks/AI Projects/2 Computer Vision/Deepfake Detection Omdena/4 Model Training/DF_EfficientB5_90.h5')"],"metadata":{"id":"pDhvZolfz6Iu"},"id":"pDhvZolfz6Iu","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 8.4 Build a Docker Image "],"metadata":{"id":"eqB_u3NYzmVh"},"id":"eqB_u3NYzmVh"},{"cell_type":"code","source":["FROM python:3.8-slim-buster\n","\n","# Set the working directory to /app\n","WORKDIR /app\n","\n","# Copy the current directory contents into the container at /app\n","COPY . /app\n","\n","# Install the required packages\n","RUN pip install --no-cache-dir -r requirements.txt\n","\n","# Set the environment variable for Flask\n","ENV FLASK_APP=app.py\n","\n","# Expose port 5000 for Flask\n","EXPOSE 5000\n","\n","# Run the Flask application\n","CMD [\"flask\", \"run\", \"--host=0.0.0.0\"]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":131},"id":"5uGhy-qm0EVv","executionInfo":{"status":"error","timestamp":1680671279241,"user_tz":-120,"elapsed":285,"user":{"displayName":"Abdelrahman Katkat","userId":"13800345600658892031"}},"outputId":"aef1229e-ef3f-41b8-d83c-bdb91eedfc30"},"id":"5uGhy-qm0EVv","execution_count":null,"outputs":[{"output_type":"error","ename":"SyntaxError","evalue":"ignored","traceback":["\u001b[0;36m  File \u001b[0;32m\"<ipython-input-11-b75078e495cc>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    FROM python:3.8-slim-buster\u001b[0m\n\u001b[0m         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"]}]},{"cell_type":"markdown","source":["## 8.5 Push the Docker Image\n","\n","Push the Docker image to a container registry such as Docker Hub or Amazon ECR. Here are the general steps to push a Docker image to Docker Hub:\n","Log in to Docker Hub using the docker login command.\n","Tag the Docker image with your Docker Hub username and repository name using the docker tag command.\n","Push the Docker image to Docker Hub using the docker push command."],"metadata":{"id":"oiMpU_Rv0PBz"},"id":"oiMpU_Rv0PBz"},{"cell_type":"code","source":[],"metadata":{"id":"8N2j4xJU0ITw"},"id":"8N2j4xJU0ITw","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 8.6 Deploy the Docker Container\n","\n","Deploy the Docker container to a cloud-based service such as Amazon ECS, AWS Fargate, or Kubernetes. The specific steps for deploying a Docker container to a cloud service will depend on the service you choose. You will typically need to create a cluster, define a task or deployment that specifies the Docker image and resource requirements, and expose the endpoint to the internet."],"metadata":{"id":"A1mwzF4x0WbO"},"id":"A1mwzF4x0WbO"},{"cell_type":"markdown","source":["# 9 New Information"],"metadata":{"id":"zo0wRq540dFY"},"id":"zo0wRq540dFY"},{"cell_type":"markdown","source":["# 9.1 Joblib\n","joblib is a library in Python that provides tools to save Python objects to disk and load them back to memory efficiently, which is particularly useful for objects that are expensive to compute. It is often used in machine learning for saving trained models, so they can be reused without having to train them from scratch each time. Joblib also provides a simple way to parallelize CPU-bound tasks using multiple cores on the same machine."],"metadata":{"id":"CzhULYpAfWNG"},"id":"CzhULYpAfWNG"},{"cell_type":"code","source":["import joblib\n","\n","# create a Python object\n","my_list = [1, 2, 3, 4, 5]\n","\n","# save the object to a file\n","joblib.dump(my_list, 'my_list.joblib')\n","\n","# load the object from the file\n","loaded_list = joblib.load('my_list.joblib')\n","\n","# print the loaded object\n","print(loaded_list)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UGnLoqvofUHw","executionInfo":{"status":"ok","timestamp":1681437590878,"user_tz":-120,"elapsed":22,"user":{"displayName":"Abdelrahman Katkat","userId":"13800345600658892031"}},"outputId":"5fce630c-e474-4f5d-f6bb-90799f11ad73"},"id":"UGnLoqvofUHw","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[1, 2, 3, 4, 5]\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"SbJiVjrVfUgO"},"id":"SbJiVjrVfUgO","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"},"colab":{"provenance":[],"collapsed_sections":["f86e8755","45b78dd0"]}},"nbformat":4,"nbformat_minor":5}